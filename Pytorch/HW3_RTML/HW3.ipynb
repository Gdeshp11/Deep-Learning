{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f67c5634910>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "torch.set_printoptions(edgeitems=2)\n",
    "torch.manual_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane','automobile','bird','cat','deer',\n",
    "               'dog','frog','horse','ship','truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "data_path = './'\n",
    "cifar = datasets.CIFAR10(\n",
    "    data_path, train=True, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_val = datasets.CIFAR10(\n",
    "    data_path, train=False, download=True,\n",
    "    transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.4915, 0.4823, 0.4468),\n",
    "                             (0.2470, 0.2435, 0.2616))\n",
    "    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 7, 8: 8, 9: 9}\n",
    "class_names = [\n",
    "    'airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse',\n",
    "    'ship', 'truck'\n",
    "]\n",
    "cifar10 = [(img, label_map[label]) for img, label in cifar]\n",
    "cifar10_val = [(img, label_map[label]) for img, label in cifar_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training on device cuda:0.\n"
     ]
    }
   ],
   "source": [
    "# defining training device to be GPU if available\n",
    "device = (torch.device('cuda:0') if torch.cuda.is_available()\n",
    "          else torch.device('cpu'))\n",
    "print(f\"Training on device {device}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.act1 = nn.Tanh()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(16, 8, kernel_size=3, padding=1)\n",
    "        self.act2 = nn.Tanh()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(8 * 8 * 8, 32)\n",
    "        self.act3 = nn.Tanh()\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.pool1(self.act1(self.conv1(x)))\n",
    "        out = self.pool2(self.act2(self.conv2(out)))\n",
    "        out = out.view(-1, 8 * 8 * 8) # <1>\n",
    "        out = self.act3(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "def training_loop(n_epochs, optimizer, model, loss_fn, train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)  # <1>\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            loss_train += loss.item()\n",
    "\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 14:40:03.701275 Epoch 1, Training loss 2.027220543845535\n",
      "2022-03-26 14:40:31.382359 Epoch 10, Training loss 1.1571621502299443\n",
      "2022-03-26 14:40:58.910190 Epoch 20, Training loss 0.9849083702582533\n",
      "2022-03-26 14:41:24.144920 Epoch 30, Training loss 0.9045757724119879\n",
      "2022-03-26 14:41:49.082989 Epoch 40, Training loss 0.8508908647253081\n",
      "2022-03-26 14:42:19.924078 Epoch 50, Training loss 0.8095403331548662\n",
      "2022-03-26 14:42:51.460231 Epoch 60, Training loss 0.7747487638460095\n",
      "2022-03-26 14:43:23.016442 Epoch 70, Training loss 0.7476884175825607\n",
      "2022-03-26 14:43:57.386794 Epoch 80, Training loss 0.7258681955239962\n",
      "2022-03-26 14:44:24.808018 Epoch 90, Training loss 0.7056385786332133\n",
      "2022-03-26 14:44:51.788576 Epoch 100, Training loss 0.683824774897312\n",
      "2022-03-26 14:45:18.550083 Epoch 110, Training loss 0.6696787827536274\n",
      "2022-03-26 14:45:45.399899 Epoch 120, Training loss 0.6542487403239741\n",
      "2022-03-26 14:46:11.674735 Epoch 130, Training loss 0.6391846546355415\n",
      "2022-03-26 14:46:37.394481 Epoch 140, Training loss 0.6262816590878665\n",
      "2022-03-26 14:47:07.489875 Epoch 150, Training loss 0.6161514572475267\n",
      "2022-03-26 14:47:44.342362 Epoch 160, Training loss 0.6032277544593567\n",
      "2022-03-26 14:48:21.352558 Epoch 170, Training loss 0.5934137429117852\n",
      "2022-03-26 14:48:58.429118 Epoch 180, Training loss 0.5863527183032706\n",
      "2022-03-26 14:49:35.707420 Epoch 190, Training loss 0.5771262575598324\n",
      "2022-03-26 14:50:05.720262 Epoch 200, Training loss 0.56723197349502\n",
      "2022-03-26 14:50:30.423041 Epoch 210, Training loss 0.5610374169009725\n",
      "2022-03-26 14:50:55.596536 Epoch 220, Training loss 0.555763411666731\n",
      "2022-03-26 14:51:20.206033 Epoch 230, Training loss 0.5492639202634086\n",
      "2022-03-26 14:51:45.080049 Epoch 240, Training loss 0.5425339140131346\n",
      "2022-03-26 14:52:10.436509 Epoch 250, Training loss 0.5388880422353135\n",
      "2022-03-26 14:52:35.666234 Epoch 260, Training loss 0.5308402960791307\n",
      "2022-03-26 14:53:00.703001 Epoch 270, Training loss 0.5289850595890714\n",
      "2022-03-26 14:53:25.490728 Epoch 280, Training loss 0.5232317924804395\n",
      "2022-03-26 14:53:50.682317 Epoch 290, Training loss 0.5200038961018137\n",
      "2022-03-26 14:54:15.144434 Epoch 300, Training loss 0.5129476313090995\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "\n",
    "model = Net()  # <1>\n",
    "model.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy train: 0.81\n",
      "Accuracy val: 0.62\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                           shuffle=False)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():  # <1>\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)  # <1>\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <2>\n",
    "                total += labels.shape[0]  # <3>\n",
    "                correct += int((predicted == labels).sum())  # <4>\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "\n",
    "validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), data_path + 'cifar10_cnn.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model = Net()  # <1>\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'cifar10_cnn.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, n_chans1=32):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(n_chans1, n_chans1 // 2, kernel_size=3,\n",
    "                               padding=1)\n",
    "        self.conv3 = nn.Conv2d(n_chans1 // 2, n_chans1 // 2,\n",
    "                               kernel_size=3, padding=1)\n",
    "        self.fc1 = nn.Linear(4 * 4 * n_chans1 // 2, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv2(out)), 2)\n",
    "        out = F.max_pool2d(torch.relu(self.conv3(out)), 2)\n",
    "        out = out.view(-1, 4 * 4 * self.n_chans1 // 2)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-26 14:54:20.071006 Epoch 1, Training loss 2.257882063955907\n",
      "2022-03-26 14:54:49.831032 Epoch 10, Training loss 1.1814646738417007\n",
      "2022-03-26 14:55:22.800317 Epoch 20, Training loss 0.9264471422680809\n",
      "2022-03-26 14:55:55.664531 Epoch 30, Training loss 0.8000339915228012\n",
      "2022-03-26 14:56:28.251022 Epoch 40, Training loss 0.7217235721057028\n",
      "2022-03-26 14:57:01.280099 Epoch 50, Training loss 0.6661967449938245\n",
      "2022-03-26 14:57:34.403194 Epoch 60, Training loss 0.6293703287535006\n",
      "2022-03-26 14:58:07.668683 Epoch 70, Training loss 0.6000132293957273\n",
      "2022-03-26 14:58:41.569885 Epoch 80, Training loss 0.5732203877109396\n",
      "2022-03-26 14:59:14.704272 Epoch 90, Training loss 0.5535791661695141\n",
      "2022-03-26 14:59:47.318157 Epoch 100, Training loss 0.5342110112843002\n",
      "2022-03-26 15:00:20.276174 Epoch 110, Training loss 0.5179338953874605\n",
      "2022-03-26 15:00:53.029716 Epoch 120, Training loss 0.5045836665250761\n",
      "2022-03-26 15:01:25.382817 Epoch 130, Training loss 0.49035039004843556\n",
      "2022-03-26 15:01:57.587998 Epoch 140, Training loss 0.48087421223483123\n",
      "2022-03-26 15:02:30.192508 Epoch 150, Training loss 0.4697704186562992\n",
      "2022-03-26 15:03:02.980809 Epoch 160, Training loss 0.4600535425955377\n",
      "2022-03-26 15:03:35.938150 Epoch 170, Training loss 0.44920030621159107\n",
      "2022-03-26 15:04:08.839238 Epoch 180, Training loss 0.4419710745706278\n",
      "2022-03-26 15:04:42.077581 Epoch 190, Training loss 0.4357017049056185\n",
      "2022-03-26 15:05:14.782163 Epoch 200, Training loss 0.42270544683918015\n",
      "2022-03-26 15:05:47.789537 Epoch 210, Training loss 0.4178700184883059\n",
      "2022-03-26 15:06:20.773267 Epoch 220, Training loss 0.40859981521468636\n",
      "2022-03-26 15:06:53.532024 Epoch 230, Training loss 0.40496657826863897\n",
      "2022-03-26 15:07:26.352275 Epoch 240, Training loss 0.3986544143551451\n",
      "2022-03-26 15:08:00.256482 Epoch 250, Training loss 0.3915678915922599\n",
      "2022-03-26 15:08:33.691444 Epoch 260, Training loss 0.39168027714085396\n",
      "2022-03-26 15:09:07.036640 Epoch 270, Training loss 0.383771376355606\n",
      "2022-03-26 15:09:40.210508 Epoch 280, Training loss 0.38335300696170543\n",
      "2022-03-26 15:10:12.636735 Epoch 290, Training loss 0.376002817698147\n",
      "2022-03-26 15:10:46.487560 Epoch 300, Training loss 0.3715705065737905\n",
      "Accuracy train: 0.83\n",
      "Accuracy val: 0.68\n"
     ]
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(cifar10, batch_size=64,\n",
    "                                           shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(cifar10_val, batch_size=64,\n",
    "                                         shuffle=False)\n",
    "all_acc_dict = collections.OrderedDict()\n",
    "\n",
    "model = Net2(n_chans1=32)  # <1>\n",
    "model.to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "def validate(model, train_loader, val_loader):\n",
    "    accdict = {}\n",
    "    for name, loader in [(\"train\", train_loader), (\"val\", val_loader)]:\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, labels in loader:\n",
    "                imgs = imgs.to(device=device)\n",
    "                labels = labels.to(device=device)\n",
    "                outputs = model(imgs)\n",
    "                _, predicted = torch.max(outputs, dim=1) # <1>\n",
    "                total += labels.shape[0]\n",
    "                correct += int((predicted == labels).sum())\n",
    "\n",
    "        print(\"Accuracy {}: {:.2f}\".format(name , correct / total))\n",
    "        accdict[name] = correct / total\n",
    "    return accdict\n",
    "\n",
    "all_acc_dict[\"depth\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), data_path + 'cifar10_cnn_2.pt')\n",
    "\n",
    "loaded_model = Net2()  # <1>\n",
    "loaded_model.load_state_dict(torch.load(data_path\n",
    "                                        + 'cifar10_cnn_2.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, n_chans):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv = nn.Conv2d(n_chans, n_chans, kernel_size=3,\n",
    "                              padding=1, bias=False)  # <1>\n",
    "        self.batch_norm = nn.BatchNorm2d(num_features=n_chans)\n",
    "        torch.nn.init.kaiming_normal_(self.conv.weight,\n",
    "                                      nonlinearity='relu')  # <2>\n",
    "        torch.nn.init.constant_(self.batch_norm.weight, 0.5)\n",
    "        torch.nn.init.zeros_(self.batch_norm.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.batch_norm(out)\n",
    "        out = torch.relu(out)\n",
    "        return out + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NetResDeep(nn.Module):\n",
    "    def __init__(self, n_chans1=32, n_blocks=10):\n",
    "        super().__init__()\n",
    "        self.n_chans1 = n_chans1\n",
    "        self.conv1 = nn.Conv2d(3, n_chans1, kernel_size=3, padding=1)\n",
    "        self.resblocks = nn.Sequential(\n",
    "            *(n_blocks * [ResBlock(n_chans=n_chans1)]))\n",
    "        self.fc1 = nn.Linear(8 * 8 * n_chans1, 32)\n",
    "        self.fc2 = nn.Linear(32, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = F.max_pool2d(torch.relu(self.conv1(x)), 2)\n",
    "        out = self.resblocks(out)\n",
    "        out = F.max_pool2d(out, 2)\n",
    "        out = out.view(-1, 8 * 8 * self.n_chans1)\n",
    "        out = torch.relu(self.fc1(out))\n",
    "        out = self.fc2(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 17:13:50.595849 Epoch 1, Training loss 1.654699182418911\n",
      "2022-03-27 17:15:22.048389 Epoch 10, Training loss 0.8983723106591598\n",
      "2022-03-27 17:17:07.124595 Epoch 20, Training loss 0.707053209700243\n",
      "2022-03-27 17:18:52.359878 Epoch 30, Training loss 0.5809508525120938\n",
      "2022-03-27 17:20:38.285898 Epoch 40, Training loss 0.47596838512002965\n",
      "2022-03-27 17:22:42.974417 Epoch 50, Training loss 0.387585387720019\n",
      "2022-03-27 17:24:27.247967 Epoch 60, Training loss 0.3160270772245534\n",
      "2022-03-27 17:26:18.784138 Epoch 70, Training loss 0.25759424888493154\n",
      "2022-03-27 17:28:00.985377 Epoch 80, Training loss 0.20874030054892267\n",
      "2022-03-27 17:29:43.020550 Epoch 90, Training loss 0.15959179033632473\n",
      "2022-03-27 17:31:25.458214 Epoch 100, Training loss 0.13916741610716676\n",
      "2022-03-27 17:33:13.496896 Epoch 110, Training loss 0.12278354054083453\n",
      "2022-03-27 17:34:55.474209 Epoch 120, Training loss 0.08534812689056177\n",
      "2022-03-27 17:36:35.846895 Epoch 130, Training loss 0.08421751653032421\n",
      "2022-03-27 17:38:17.621775 Epoch 140, Training loss 0.09784017827854875\n",
      "2022-03-27 17:39:59.791787 Epoch 150, Training loss 0.035534196480860945\n",
      "2022-03-27 17:41:41.761115 Epoch 160, Training loss 0.05073595021897808\n",
      "2022-03-27 17:43:21.297161 Epoch 170, Training loss 0.04637203016824534\n",
      "2022-03-27 17:45:02.549748 Epoch 180, Training loss 0.015267064626676404\n",
      "2022-03-27 17:46:45.249390 Epoch 190, Training loss 0.007880723173551433\n",
      "2022-03-27 17:48:25.179011 Epoch 200, Training loss 0.11928208125576549\n",
      "2022-03-27 17:50:04.817744 Epoch 210, Training loss 0.02415281824518681\n",
      "2022-03-27 17:51:45.099431 Epoch 220, Training loss 0.0181829921488562\n",
      "2022-03-27 17:53:24.609129 Epoch 230, Training loss 0.09490925068800311\n",
      "2022-03-27 17:55:05.941057 Epoch 240, Training loss 0.015459082824775896\n",
      "2022-03-27 17:56:45.423306 Epoch 250, Training loss 0.004727369523086154\n",
      "2022-03-27 17:58:28.435298 Epoch 260, Training loss 0.0043256930298888885\n",
      "2022-03-27 18:00:11.010299 Epoch 270, Training loss 0.004094405131551641\n",
      "2022-03-27 18:01:55.255917 Epoch 280, Training loss 0.002631676945463329\n",
      "2022-03-27 18:03:47.549779 Epoch 290, Training loss 0.002120731916861273\n",
      "2022-03-27 18:05:40.826836 Epoch 300, Training loss 0.0027421565581699048\n",
      "Accuracy train: 1.00\n",
      "Accuracy val: 0.66\n"
     ]
    }
   ],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "\n",
    "all_acc_dict[\"res deep\"] = validate(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop_l2reg(n_epochs, optimizer, model, loss_fn,\n",
    "                        train_loader):\n",
    "    for epoch in range(1, n_epochs + 1):\n",
    "        loss_train = 0.0\n",
    "        for imgs, labels in train_loader:\n",
    "            imgs = imgs.to(device=device)\n",
    "            labels = labels.to(device=device)\n",
    "            outputs = model(imgs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            l2_lambda = 0.001\n",
    "            l2_norm = sum(p.pow(2.0).sum()\n",
    "                          for p in model.parameters())  # <1>\n",
    "            loss = loss + l2_lambda * l2_norm\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            loss_train += loss.item()\n",
    "        if epoch == 1 or epoch % 10 == 0:\n",
    "            print('{} Epoch {}, Training loss {}'.format(\n",
    "                datetime.datetime.now(), epoch,\n",
    "                loss_train / len(train_loader)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-27 18:43:47.136991 Epoch 1, Training loss 1.874712833967965\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb Cell 20'\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=1'>2</a>\u001b[0m optimizer \u001b[39m=\u001b[39m optim\u001b[39m.\u001b[39mSGD(model\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39m\u001b[39m3e-3\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=2'>3</a>\u001b[0m loss_fn \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mCrossEntropyLoss()\n\u001b[0;32m----> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=4'>5</a>\u001b[0m training_loop_l2reg(\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=5'>6</a>\u001b[0m     n_epochs \u001b[39m=\u001b[39;49m \u001b[39m300\u001b[39;49m,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=6'>7</a>\u001b[0m     optimizer \u001b[39m=\u001b[39;49m optimizer,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=7'>8</a>\u001b[0m     model \u001b[39m=\u001b[39;49m model,\n\u001b[1;32m      <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=8'>9</a>\u001b[0m     loss_fn \u001b[39m=\u001b[39;49m loss_fn,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=9'>10</a>\u001b[0m     train_loader \u001b[39m=\u001b[39;49m train_loader,\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=10'>11</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000020vscode-remote?line=11'>12</a>\u001b[0m all_acc_dict[\u001b[39m\"\u001b[39m\u001b[39mResnet l2 reg\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m validate(model, train_loader, val_loader)\n",
      "\u001b[1;32m/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb Cell 19'\u001b[0m in \u001b[0;36mtraining_loop_l2reg\u001b[0;34m(n_epochs, optimizer, model, loss_fn, train_loader)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000019vscode-remote?line=13'>14</a>\u001b[0m loss \u001b[39m=\u001b[39m loss \u001b[39m+\u001b[39m l2_lambda \u001b[39m*\u001b[39m l2_norm\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000019vscode-remote?line=15'>16</a>\u001b[0m optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000019vscode-remote?line=16'>17</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000019vscode-remote?line=17'>18</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell://wsl%2Bubuntu/home/gandhar/HW0_RTML/git/Pytorch/dlwpt-code-master/p1ch8/P1.ipynb#ch0000019vscode-remote?line=19'>20</a>\u001b[0m loss_train \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[0;32m--> <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/autograd/__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[0;32m--> <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    <a href='file:///home/gandhar/.local/lib/python3.8/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = NetResDeep(n_chans1=32, n_blocks=10).to(device=device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=3e-3)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "training_loop_l2reg(\n",
    "    n_epochs = 300,\n",
    "    optimizer = optimizer,\n",
    "    model = model,\n",
    "    loss_fn = loss_fn,\n",
    "    train_loader = train_loader,\n",
    ")\n",
    "all_acc_dict[\"Resnet l2 reg\"] = validate(model, train_loader, val_loader)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
